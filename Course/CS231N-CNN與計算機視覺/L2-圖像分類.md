# 圖像分類

## 目錄


<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [圖像分類](#圖像分類)
    - [目錄](#目錄)
    - [本章總結](#本章總結)
    - [上課內容](#上課內容)
    - [數據驅動方法](#數據驅動方法)
        - [圖像分類問題](#圖像分類問題)
    - [K-NN (K-最近鄰)](#k-nn-k-最近鄰)
        - [演算法](#演算法)
        - [計算距離](#計算距離)
            - [L1 (Manhattan) distance 曼哈頓距離](#l1-manhattan-distance-曼哈頓距離)
            - [L2 (Euclidean) distance 歐式距離](#l2-euclidean-distance-歐式距離)
            - [L1 L2 比較圖](#l1-l2-比較圖)
        - [KNN 的問題](#knn-的問題)
        - [KNN 線上範例](#knn-線上範例)
    - [超參數](#超參數)
        - [如何根據問題和數據選擇這些超參數](#如何根據問題和數據選擇這些超參數)
    - [線性分類](#線性分類)
    - [可參考資料](#可參考資料)

<!-- /code_chunk_output -->

---

## 本章總結

由於 [圖像分類問題](#圖像分類問題) 使我們很難靠 演算法 + 特徵 去辨識每個種類，因此只有靠機器自行學習

常見於機器學習中的 [KNN](#k-nn-k-最近鄰)，由於[KNN 本身的問題](#knn-的問題)，所以對於計算機視覺的效果不是那麼好
因此目前主流是 CNN，而 [線性分類](#線性分類) 則是神經網路的基礎

最後也說明了如何尋找 [超參數](#超參數)

---

## 上課內容

如果沒有特別註明，所有圖片皆是從 該課程的 投影片出來的

## 數據驅動方法

### 圖像分類問題

與人不同，計算機看到的都是數字，因此圖像分類問題有幾個難點

1. 光影變化
2. 自身變形 (動物動作會改變)
3. 遮擋
4. 跟背景近似的顏色
5. 個體不同( 例如 貓有不同毛色)

這些問題造成我們很難使用明確的程式碼來告訴機器如何辨識圖片
即便某些物體能夠找出固定的特徵，但是如果要新增一個類別，就要再次為那個類別量身打造特徵及演算法
這是件耗時耗力的工作
![使用特徵的圖片辨識](https://i.imgur.com/UjWWNx7.png)

因此，近年來，由於數據量的增多以及計算資源的提升
使得我們可以直接使用 data 去訓練 model (數據驅動)，而後使用 model 來預測圖片

## K-NN (K-最近鄰)

### 演算法

機器學習其中一個常見的學習方法便是 KNN
其主要算法是

1. 儲存所有資料
2. 找出距離 待預測點 最近的 k 個點
3. 回傳該 k 個點中，數量最多的類別

<iframe width="640" height="360" src="https://www.youtube.com/embed/nEivmykTijo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

![執行結果](https://i.imgur.com/pZV3HH7.png)

上圖我們發現
當 k = 1 時非常不平滑，這也是其缺點
k = N 可以緩解這問題
且 可以加上 權重等等

而白色的區域則是因為有多個相等的多數表決結果(例如 A B C 各一票)

### 計算距離

距離選擇是根據對隱含 function 的假設
常見的有 L1 、 L2 兩種

#### L1 (Manhattan) distance 曼哈頓距離

L1 曼哈頓距離 - 旋轉使 L1 距離改變
如果向量中的一些值有特殊的意義，建議用 L1
$$d_1(I_1,I_2)=\sum_P|I^P_1-I^P_2|$$

#### L2 (Euclidean) distance 歐式距離

L2 歐式距離 - 旋轉不使 L2 距離改變
如果不知其中元素的意義，建議用 L2
$$d_2(I_1,I_2)=\sqrt{\sum_P(I^P_1-I^P_2)^2}$$

#### L1 L2 比較圖

![L1 L2 比較圖](https://i.imgur.com/VIXV7sV.png)
如果知道向量元素的實際意義， L1 "可能" 比 L2 好

### KNN 的問題

KNN 在圖像分類上很少用，因為

1. 預測運算慢
2. L1 L2 無法很好評估圖片

例如右邊三張圖片都有相同的 L2 loss
![L2 例子](https://i.imgur.com/BpewBVm.png)
KNN 另一個問題是維度災難
因為只有資料在空間中密集，KNN才具意義，不然可能空間相鄰很遠，但因資料稀少，所以是最近的，因此說是一樣的類別。
但如果要保持這種密集程度，資料就要指數倍成長，這非常困難
![維度災難](https://i.imgur.com/YwQXz9H.png)

此外KNN 的訓練時間 O(1)
但測試時間是 O(N)，這也是他的缺點，因為這個特性，使得他比較無法跑在嵌入式系統等小電腦中

### KNN 線上範例

http://vision.stanford.edu/teaching/cs231n-demos/knn/

## 超參數

超參數 (Hyperparameters) 是指不能從資料中學到的參數
例如 KNN 中的 k 或 距離 function

### 如何根據問題和數據選擇這些超參數

如何選擇參數

1. 選擇在訓練集上表現最好的超參數? 不，會過耦合
2. train / test ?  不，可能只能符合 test set，在未知數據上可能表現很糟，無法代表沒看過的數據
3. train / Validation / test - 用吧!
4. train / CV / test - 小資料集常用 ， DL 比較不常，因為深度學習計算開銷太大

![三種評估方式](https://i.imgur.com/M4qI4qY.png)
![CV](https://i.imgur.com/6Z3m0V3.png)

數據集的收集對於深度學習很重要，請確定 train / Validation / test 三個資料集都是使用相同方式在相同時間長度蒐集的資料，通常會先一次蒐集完所有資料，再隨機劃分數據集

如果選定了模型，可以重新在整個訓練集上訓練，會增加一些正確率

## 線性分類

線性分類對於深度學習而言，就如同積木城堡的積木，不僅是基礎，也能夠自由拆裝
![疊積木示意圖](https://i.imgur.com/WYgNGYQ.png)

底下是最簡單的線性函式
x 是輸入
W 以及 b 是參數
我們的目標是要調整 W 跟 b 使這個函式能夠輸出正確的 label
$f = Wx + b$

例如 對於 CIFAR10 這個資料集而言，輸入是 32 * 32 的圖片，輸出是類別，總共有 10 個類別
因此我們的輸入大小會是  32 * 32 * 3(3 是R G B 三色)，輸出是 10 個類別的分數，我們將取分數最大的類別作為這張圖片的預測結果
![例子](https://i.imgur.com/WT8lb50.png)
![例子](https://i.imgur.com/0MQbJOS.png)

因為可以逆向計算，所以這裡透過逆向計算做可視化
可以得到
![可視化](https://i.imgur.com/NIaffY5.png)

> 我們發現有些類別 (例如馬) 出現了不存在的圖片 (雙頭馬)
> 這是因為線性分類的限制，每個類別只能學到一個模板，所以可能是取平均
> 但是這在神經網路就沒這限制
> 這部分我不是很確定@@ by @sappy

從空間上來看，線性分類就是對維度空間的切割
![切割示意圖](https://i.imgur.com/rrOLj6p.png)

因此無法很好的處理下面三張圖
![線性分類無法區分的圖](https://i.imgur.com/3pMhJVv.png)

## 可參考資料

[CS231n 2017 學習筆記01——KNN（K-Nearest Neighbors）](https://hk.saowen.com/a/6d5e6bfffb0d4cb684299298bfd606f7db24a7f58e405092b86ea43b543d6fd7)