# 深度學習軟體

# 目錄

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [深度學習軟體](#深度學習軟體)
- [目錄](#目錄)
- [CPU vs GPU](#cpu-vs-gpu)
	- [硬碟讀取資料會是限制速度的瓶頸](#硬碟讀取資料會是限制速度的瓶頸)
	- [深度學習框架](#深度學習框架)
		- [深度學習框架的三個方便之處](#深度學習框架的三個方便之處)
		- [範例](#範例)
		- [Tensorflow](#tensorflow)
		- [Keras](#keras)
		- [Pytorch](#pytorch)
	- [動靜態圖比較](#動靜態圖比較)
		- [Caffe](#caffe)
		- [Caffe 2](#caffe-2)

<!-- /code_chunk_output -->


# CPU vs GPU

![](https://i.imgur.com/Mz4eJ6D.png)

GPU 適合平行處理，像如矩陣乘法(畢竟是針對影像處理設計的)


* CUDA - 寫高效能的 cod 很難
    * 包裝層
        * cuBLAS - 矩陣操作
        * cuFFT
        * cuDNN - 深度學習 (跟手寫 CUDA 效能差三倍)

* OpenCL
    * 效能不比 CUDA 好
    * 但跨平台

## 硬碟讀取資料會是限制速度的瓶頸

所以要

1. 一口氣讀進 GPU RAM
2. 用 SSD 取代 HDD
3. 先放入 RAM 再放入 GPU RAM

## 深度學習框架

![](https://i.imgur.com/w438Yp8.png)

今天主講 PyTorch 及 Tensorflow

### 深度學習框架的三個方便之處

1. 更容易建立複雜的計算圖(Computational graphs)
2. 更容易計算出梯度
3. 能在 GPU 上跑

### 範例

用兩層 NN + ReLU + L2 loss

### Tensorflow

1. 定義計算圖
2. 執行

placeholder -> 輸入節點
gradients -> 導數

在 Sesion 中才開始執行計算
run 才真正開始跑

但是 placeholder 在 GPU 計算時會在 CPU 及 GPU 間互傳
所以會非常耗時，因此要用 Variable 解決 (都在 GPU 中，但要記得初始化)
此外，更新參數也要記得寫入計算圖中
用 global_variables_initializer() 初始化變數後再去訓練

有個嚴重的問題
因為 TF 只做有效的計算，因此如果只跟他要 loss，他就真的只算到 loss，不會往下算
因此必須明確跟他要求要計算 新的權重
但是這樣又回在 CPU 與 GPU 間傳來傳去
因此用 dummy_node (仿製節點)，來解決這問題

可以再用 optimizer 簡化程式碼(43分鐘)，但是我不太清楚 QQ

TF 有一些高級函式庫

### Keras

Tensorflow or Theano 的包裝

此外也有很多其他高級函式庫

Tensorflow 的語法跟 Theano 很像

### Pytorch

* Tensor - 類似 numpy
* Variable 節點
* Module 神經網路

用 .cuda 移到 gpu

x 是 Variable 的話
x.data -> 資料 (Tensor)
x.grad -> 梯度
x.grad.data -> 梯度 的 (Tensor)

Variabel 跟 Tensor 有相同 API

傳入參數時記得要 把 require_grad 設為 false
loss 計算時要記得直接用 data 算

Pytorch 是直接運算，且效果程式碼
此外，可以自己寫 forward 及 backward

nn 是高級封裝包
optim -> 優化方法

自定義 nn Module
要先繼承 nn Module

DataLoader -> 輔助讀取
mini-batch
multi-thread 讀取資料

Pytorch 有預先訓練的模型，且容易使用
Visdom
Pytorch 從 Torch 來

Pytorch 可以動態改變計算圖，每次計算圖都可以不同

## 動靜態圖比較

靜態圖更有效率(可以做優化)，但寫起來更複雜
動態圖稍微沒有那麼有效率

靜態圖可以序列化(固定、存在硬碟中) 計算圖，因此序列化後可以用 C++ 等其他語言 部屬執行，不需要原本的程式碼
但動態圖就不行

動態圖在需要條件判斷的動作比較方便
Loop 也比較方便 例如 RNN 、 Recursive Networks

有新的 Library 叫做 Tensorflow Fold，類似 Pytorch，但是據說語法有點怪

動態圖方便在 RNN 、 Recursive Networks(NLP)、Neural Modular Network(可視化問答系統) 就是針對不同問題用專門的網路下去解，但目前不是主流
例如問物種 就有個計算數量的物種 + NLP網路
問顏色就有 就有個計算數量的網路 + NLP網路

此外動態圖很自由，所以應該還可以做些其他事情

### Caffe

1. 轉換資料格式
2. 用 prototxt 畫計算圖
3. 定義參數
4. 執行

很好產品化，但近年研究用的有點少

### Caffe 2

靜態圖
可用 python 定義網路，也可轉成 prototxt


Tensorflow 包山包海

Pytorch 針對研究
Caffee 2 針對 產品化 